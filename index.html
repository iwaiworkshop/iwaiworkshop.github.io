<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>2nd International Workshop on Active Inference</title>

  <!-- Bootstrap core CSS -->
  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet">
  <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic,700italic" rel="stylesheet" type="text/css">

  <!-- Custom styles for this template -->
  <link href="css/landing-page.css" rel="stylesheet">

  <style>
	 header.masthead {
	  position: relative;
	  background-color: #343a40;
	  background: url("../img/bilbao-masthead.jpg") no-repeat center center;
	  background-size: cover;
	  padding-top: 8rem;
	  padding-bottom: 8rem;
	  text-shadow: 1px 1px 2px #212529;
	}
  </style>
</head>

<body>

  <!-- Navigation -->
  <nav class="navbar navbar-expand navbar-light bg-light static-top">
    <div class="container">
      <a class="navbar-brand" href="index.html">2nd International Workshop on Active Inference</a>
      <ul class="navbar-nav">
      	<li class="nav-item"><a class="nav-link" href="#">Home</a></li>
      	<li class="nav-item"><a class="nav-link" href="#call-for-papers">Call for papers</a></li>
      	<li class="nav-item"><a class="nav-link" href="#programme">Programme</a></li>
      </ul>
    </div>
  </nav>

  <!-- Masthead -->
  <header class="masthead text-white text-center">
    <div class="overlay"></div>
    <div class="container">
      <div class="row">
        <div class="col-xl-9 mx-auto">
          <h3>IWAI 2021</h3>
          <h1 class="mb-5">2nd International Workshop<br />on Active Inference</h1>
          <h4>13/17 September 2021 - virtually</h4>
          <h4>In Conjunction with ECML/PKDD 2021</h4>
        </div>
      </div>
    </div>
  </header>

  <section class="topsection text-left bg-light">
  	<div class="container">

  		<!-- <h4>Motivation of the workshop</h4> -->
  		<p class="mb-0">
  			The 2nd International Workshop on Active Inference wants to bring together researchers on active inference as well as related research fields in order to discuss current trends, novel results, (real-world) applications, to what extent active inference can be used in modern machine learning settings, such as deep learning, unifying the latest psychological and neurological insights and how it can be used in understanding action, optimization and choice.
  		</p>
  		<p class="mb-0">
  			Active inference is a theory of behaviour and learning that originated in neuroscience <a href="#ref1">(Friston et al., 2006)</a>. The basic assumption is that intelligent agents entertain a generative model of their environment, and their central objective is to minimize a tractable upper bound on the surprise of sensory observations, known as variational free energy. The agents do so either by updating their generative model, so that it becomes better at explaining observations (i.e. learning), or by inferring policies that will resolve their surprise (i.e. acting), for example by moving towards prior, preferred states, or by moving towards less ambiguous states <a href="#ref3">(Friston et al., 2017)</a>.
  		</p>
		<p class="mb-0">
			In the field of machine learning, the definition of free energy is also known as the (negative) evidence lower bound (ELBO) in variational Bayesian methods. In deep learning, this has become a popular method to build generative models of complex data using the variational autoencoder framework <a href="#ref4">(Kingma et al., 2014, </a><a href="#ref5">Rezende et al., 2014)</a>. For that reason, active inference has connections with the currently popular domain of reinforcement learning and intrinsic motivation <a href="#ref3">(Friston et al., 2009)</a>. In the field of complexity economy, the free energy principle is used to temper rational choice theory reformulating how agents optimize <a href="#ref6">(Morten Henriksen, 2020)</a>.
		</p>
  	</div>
  </section>

  <section class="middlesection text-left bg-light" id="programme">
    <div class="container">
      <h4>Programme</h4>

      TBA
<!--      <h5>Invited talks</h5>
      <div class="container">
        <div class="row">
          <div class="col-2">
            <img src="img/schwartenbeck.jpg" class="img-fluid" alt="Responsive image">
          </div>
          <div class="col-10">
            <p class="mb-0">
              <i>Active Learning and Active Inference in Exploration</i></br>
              Philipp Schwartenbeck
            </p>
            <p class="mb-0">
            Successful behaviour depends on the right balance between maximising reward and soliciting information about the world. I will discuss how different types of information-gain emerge when casting behaviour as surprise minimisation and planning as an inferential process. This formulation provides two distinct mechanisms for goal-directed exploration that express separable profiles of active sampling to reduce uncertainty. 'Hidden state' exploration motivates agents to sample unambiguous observations to accurately infer the (hidden) state of the world. Conversely, 'model parameter' exploration, compels agents to sample outcomes associated with high uncertainty, if they are informative for their representation of the task structure. I will try to provide an introductory illustration of the emergence of these types of 'Bayes-optimal' exploratory behaviour, termed active inference and active learning, and discuss possible future developments and experimental investigations of such implementations in artificial and biological agents. <br/>
[<a href="http://slideslive.com/38933135" target="_blank">Presentation</a>]
            </p>
          </div>
        </div>

        <div class="row">
          <div class="col-2">
            <img src="img/lowe.jpg" class="img-fluid" alt="Responsive image">
          </div>
          <div class="col-10">
            <p class="mb-0">
              <i>Putting An End to End-to-End: Gradient-Isolated Learning of Representations</i></br>
              Sindy L&#246;we
            </p>
            <p class="mb-0">
            We propose a novel deep learning method for local self-supervised representation learning that does not require labels nor end-to-end backpropagation but exploits the natural order in data instead. Inspired by the observation that biological neural networks appear to learn without backpropagating a global error signal, we split a deep neural network into a stack of gradient-isolated modules. Each module is trained to maximally preserve the information of its inputs using the InfoNCE bound from Oord et al. [2018]. Despite this greedy training, we demonstrate that each module improves upon the output of its predecessor, and that the representations created by the top module yield highly competitive results on downstream classification tasks in the audio and visual domain. The proposal enables optimizing modules asynchronously, allowing large-scale distributed training of very deep neural networks on unlabelled datasets. <br/>
[<a href="http://slideslive.com/38933137" target="_blank">Presentation</a>]
            </p>
          </div>
        </div>
      </div>


      <h5>Accepted presentations</h5>
        <p class="mb-0">
          <i>Confirmatory evidence that healthy individuals can adaptively adjust prior expectations and interoceptive precision estimates</i></br>
          Ryan Smith, Rayus Kuplicki, Adam Teed, Valerie Upshaw and Sahib S. Khalsa <br/>
          [<a href="http://slideslive.com/38933115" target="_blank">Presentation</a>]
        </p>

        <h5>Accepted posters</h5>
-->      
    </div>
  </section>

  <section class="middlesection text-left bg-light" id="call-for-papers">
  	<div class="container">
  		<h4>Call for papers</h4>
  		<p class="mb-0">
  			Papers on all subjects and applications of active inference and related research areas are welcome. Topics of interest include (but are not limited to):
  		</p>
		<ul>
			<li>Active inference</li>
			<li>(Bayesian) surprise</li>
			<li>Cognitive robotics</li>
			<li>Computational neuroscience</li>
			<li>(Deep) generative models</li>
			<li>State space models</li>
			<li>Intrinsic motivation</li>
			<li>Intelligent systems</li>
                        <li>Decision making in economics</li>
			<li>...</li>
		</ul>
		<p class="mb-0"></p>
  	</div>
  </section>

  <section class="subsection text-left bg-light">
  	<div class="container">
  		<h5>Important dates</h5>
  		<p class="mb-0">
  		    Abstract Submission Deadline: June 9th, 2021</span> <br />
                    Paper Submission Deadline: June 23rd, 2021 <br />
		    Acceptance Notification: July 28th, 2021 <br />
	            <!--Camera Ready Submission Deadline: TBA <br />-->
		    Workshop Date: September 13th or 17th, 2021
  		</p>
  	</div>
  </section>

  <section class="subsection text-left bg-light">
  	<div class="container">
  		<h5>Paper submissions</h5>
  		<p class="mb-0">
  			We welcome submissions of papers with up to 8 printed pages (excluding references) in LNCS format (<a href="https://www.springer.com/gp/computer-science/lncs/conference-proceedings-guidelines" target="_blank">click here for details</a>). Submissions will be evaluated according to their originality and relevance to the workshop, and should have an abstract of 60-100 words. Contributions should be in PDF format and submitted via Easychair (<a href="https://easychair.org/my/conference?conf=iwai2021" target="_blank">click here</a>).
  		</p>
  		<p class="mb-0">
  			<span class="text-primary">In accordance with the main conference, will apply a <b>double-blind</b> review process</span> (see also the double-blind reviewing process section below for further details). All papers need to be anonymized in the best of efforts. It is allowed to have a (non-anonymous) online pre-print. Reviewers will be asked not to search for them.
  		</p>
  	</div>
  </section>

<!--
  <section class="middlesection text-left bg-light">
  	<div class="container">
  		<h4>Registration</h4>
  		<p class="mb-0">
  			The workshop registrations will be handled by ECML/PKDD 2021 (<a href="https://ecmlpkdd2020.net/attending/registration" target="_blank">click here</a>). At least one author of each accepted paper should register for the conference.
  		</p>
  		<p class="mb-0">
  			Keep in mind: the early registration deadline for ECML/PKDD is July 20, 2020.
  		</p>
  	</div>
  </section>
-->

  <section class="middlesection text-left bg-light">
  	<div class="container">
  		<h4>Organisers</h4>
  		<p class="mb-0">
			Christopher Buckley, University of Sussex, United Kingdom <br />
                        Daniela Cialfi, University of Chieti-Pescara, Italy <br />
			Pablo Lanillos, Donders Institute for Brain, Cognition and Behaviour, Netherlands <br />
                        Maxwell Ramstead, McGill University, Canada <br />
  			Tim Verbelen, Ghent University - imec, Belgium
  		</p>
  	</div>
  </section>

  <section class="middlesection text-left bg-light">
  	<div class="container">
  		<h4>Programme committee</h4>
  		<p class="mb-0">
  			Karl Friston, University College London, United Kingdom <br />
			Noor Sajid, University College London, United Kingdom <br />
			Rosalyn Moran, King’s College London, United Kingdom <br />
			Ayca Ozcelikkale, Uppsala University, Sweden <br />
			Christoph Mathys, Aarhus University, Denmark <br />
			Glen Berseth, University of California Berkeley, USA <br />
			Casper Hesp, University of Amsterdam, Netherlands <br />
			Tim Verbelen, Ghent University - imec, Belgium <br />
			Cedric De Boom, Ghent University - imec, Belgium <br />
			Bart Dhoedt, Ghent University - imec, Belgium <br />
			Christopher Buckley, University of Sussex, United Kingdom <br />
			Alexander Tschantz, University of Sussex, United Kingdom <br />
			Maxwell Ramstead, McGill University, Canada <br />
			Pablo Lanillos, Donders Institute for Brain, Cognition and Behaviour, Netherlands <br />
                        Daniela Cialfi, University of Chieti-Pescara, Italy <br />
			Kai Ueltzh&#246;ffer, Heidelberg University, Germany <br />
			Martijn Wisse, Delft University of Technology, Netherlands
  		</p>
  	</div>
  </section>

  <section class="middlesection text-left bg-light">
  	<div class="container">
  		<h4>Previous editions</h4>
  		<p class="mb-0">
  			<a href="2020.html">2020 - Ghent (virtual)</a><br />
  		</p>
  	</div>
  </section>

  <section class="middlesection text-left bg-light">
  	<div class="container">
  		<h4>References</h4>
  		<p class="mb-0" id="ref1">
  			Karl Friston, James Kilner, Lee Harrison. A free energy principle for the brain. Journal of Physiology-Paris, Volume 100, Issues 1–3, 2006.
  		</p>
  		<p class="mb-0" id="ref2">
			Karl J. Friston, Jean Daunizeau, and Stefan J. Kiebel. Reinforcement Learning or Active Inference? PLoS ONE, 4(7), 2009.
		</p>
		<p class="mb-0" id="ref3">
			Karl J. Friston, Marco Lin, Christopher D. Frith, Giovanni Pezzulo, J. Allan Hobson, and
			Sasha Ondobaka. Active Inference, Curiosity and Insight. Neural Computation, 29(10):
			2633–2683, 2017.
		</p>
		<p class="mb-0" id="ref4">
			Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. 2nd International Conference on Learning Representations, 2014.
		</p>
		<p class="mb-0" id="ref5">
			Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. 31st International Conference on International Conference on Machine Learning, 2014.
  		</p>
		<p class="mb-0" id="ref6">
                        Henriksen, M. Variational Free energy and Economics Optimizing With Biases and Bounded Rationality. Frontier in Psychology, 11:549187, 2020.
  		</p>
  	</div>
  </section>


  <!-- Footer -->
  <footer class="footer bg-light">
    <div class="container">
      <div class="row">
        <div class="col-lg-6 h-100 text-center text-lg-left my-auto">
          <!-- <ul class="list-inline mb-2">
            <li class="list-inline-item">
              <a href="#">About</a>
            </li>
            <li class="list-inline-item">&sdot;</li>
            <li class="list-inline-item">
              <a href="#">Contact</a>
            </li>
            <li class="list-inline-item">&sdot;</li>
            <li class="list-inline-item">
              <a href="#">Terms of Use</a>
            </li>
            <li class="list-inline-item">&sdot;</li>
            <li class="list-inline-item">
              <a href="#">Privacy Policy</a>
            </li>
          </ul> -->
        </div>
<!--         <div class="col-lg-6 h-100 text-center text-lg-right my-auto">
          <ul class="list-inline mb-0">
            <li class="list-inline-item mr-3">
              <a href="#">
                <i class="fab fa-facebook fa-2x fa-fw"></i>
              </a>
            </li>
            <li class="list-inline-item mr-3">
              <a href="#">
                <i class="fab fa-twitter-square fa-2x fa-fw"></i>
              </a>
            </li>
            <li class="list-inline-item">
              <a href="#">
                <i class="fab fa-instagram fa-2x fa-fw"></i>
              </a>
            </li>
          </ul>
        </div> -->
      </div>
    </div>
  </footer>

  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

</body>

</html>
